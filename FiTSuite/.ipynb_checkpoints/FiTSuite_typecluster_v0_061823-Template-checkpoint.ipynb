{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9d381c",
   "metadata": {},
   "source": [
    "# Filament Tools Suite (FiTSuite) - typecluster v0: Empty Template\n",
    "\n",
    "Last modified: 6/18/23, DL\n",
    "\n",
    "Should just be able to run all and add your use case at the end. Parameters and inputs detailed below.\n",
    "\n",
    "Suggest making a copy of this Jupyter notebook for each dataset to be processed so the full record of processing is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65b071",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### *General instructions*\n",
    "The following is a simple way to set up an virtual environment with venv if you don't know how to:\n",
    "- Run the following commands in a new terminal, then shutdown your current jupyter notebook and server\n",
    "    - `python3 -m venv FiTSuite`\n",
    "    - `source FiTSuite/bin/activate.csh` or `source FiTSuite/bin/activate`\n",
    "    - `python3 -m pip install ipykernel`\n",
    "    - `python3 -m ipykernel install --user --name=FiTSuite`\n",
    "- Re-run the command `jupyter notebook` or `python3 -m notebook` or whatever you normally do and open this notebook\n",
    "    - Currrently incompatible with JupyterLab 4 due to plotly issues... use JupyterLab 3 e.g. `pip install jupyterlab==3`\n",
    "- Upon reopening, choose FiTSuite as Kernel by going to `Kernel > Change kernel > FiTSuite`\n",
    "\n",
    "When done running the Jupyter notebook and shutting down the server, you can deactivate the virtual environment with `deactivate` in the second terminal\n",
    "\n",
    "After you have done this once, next time just run `jupyter notebook` to get back here, and change kernel to `FiTSuite` again\n",
    "\n",
    "### *LMB instructions*\n",
    "The following is a simple way to set up an virtual environment with venv if you don't know how to:\n",
    "- Run the following commands in a new terminal from your LMB home directory:\n",
    "    - `module avail python`\n",
    "        - Should see something like \"python3/3.10.7\" or higher\n",
    "    - `module load python3/3.10.7` \n",
    "        - or whatever the latest version of python is\n",
    "    - `python3 -m venv FiTSuite`\n",
    "    - `source FiTSuite/bin/activate.csh` or `source FiTSuite/bin/activate`\n",
    "    - `python3 -m pip install jupyter notebook` (could also be pip3)\n",
    "        - Currrently incompatible with JupyterLab 4 due to plotly issues... use JupyterLab 3 e.g. `pip install jupyterlab==3`        \n",
    "    - `python3 -m ipykernel install --user --name=FiTSuite`\n",
    "- Run the command `jupyter notebook` or `python3 -m notebook` or whatever you normally do and open this notebook\n",
    "- Upon reopening, choose FiTSuite as Kernel by going to `Kernel > Change kernel > FiTSuite`\n",
    "\n",
    "When done running the Jupyter notebook and shutting down the server, you can deactivate the virtual environment with `deactivate`\n",
    "\n",
    "After you have done this once, next time just reactivate the virtual environment, run `jupyter notebook` to get back here, and make sure kernel is still set to `FiTSuite`\n",
    "\n",
    "Also, can source from my home directory -> `source lmb/home/dli/FiTSuite/bin/activate.csh`. This is likely easiest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If kernel is selected properly, should expect to see FiTSuite in the string below\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be >=3.10\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8c27e",
   "metadata": {},
   "source": [
    "### Package Installation\n",
    "FiTSuite requires a number of packages. These can be installed multiple ways, here are two:\n",
    "\n",
    "\n",
    "1. Install using the `FiTSuite_requirements.txt`, e.g.\n",
    "    ```\n",
    "    # Run this ONLY the first time you create the venv to install all required packages (and some unnecessary ones oops)\n",
    "    !python3 -m pip install -U -r FiTSuite_requirements.txt\n",
    "    # Important relatively uncommon ones are starfile, seaborn, fastcluster, plotly, ipywidgets\n",
    "    # NEED ipywidgets <= 8 because major overhaul that is not compatible with plotly\n",
    "    ```\n",
    "2. Install the missing packages explicitly, e.g. (may need to add more)\n",
    "    ```\n",
    "    # Install missing/uncommon packages\n",
    "    !pip install fastcluster\n",
    "    !pip install scikit-learn\n",
    "    !pip install umap-learn\n",
    "    !pip install starfile\n",
    "    !pip install mrcfile\n",
    "    !pip install plotly\n",
    "    !pip install seaborn\n",
    "    !pip install scikit-image\n",
    "    !pip install ipywidgets==7.7.1 jupyterlab-widgets==1.1.1 \n",
    "    # NEED ipywidgets <= 8 because major overhaul that is not compatible with plotly\n",
    "\n",
    "    ! jupyter nbextension enable --py widgetsnbextension\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51150a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages (some are for other functions in FiTSuite)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import starfile\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import fcluster, dendrogram\n",
    "import scipy\n",
    "import mrcfile\n",
    "import fastcluster\n",
    "import matplotlib.patches as patches\n",
    "import IPython.display\n",
    "from skimage import filters\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# Packages for interactive functionality\n",
    "from ipywidgets import widgets, interactive, HBox, VBox\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Uncomment only if you want to either use KMeans or compute dimensionality reduction plots\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633829c",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eabbc0",
   "metadata": {},
   "source": [
    "- File I/O\n",
    "    - read_particle_star(filename: str) -> (pd.DataFrame, collections.OrderedDict)\n",
    "    - read_models_star(filename: str) -> (pd.DataFrame, collections.OrderedDict)\n",
    "    - read_mrcs_file(filename: str) -> (list, np.recarray)\n",
    "    - read_mrc_file(filename: str) -> (np.ndarray, np.recarray)\n",
    "    - write_particle_star(all_data: collections.OrderedDict, particle_df: pd.DataFrame, filename: str, overwrite: bool = True)\n",
    "    - write_models_star(classes_df: pd.DataFrame, filename: str, overwrite: bool = True)\n",
    "- Add columns to individual particle_df\n",
    "    - hash_particle_df(particle_df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame\n",
    "    - add_classnumber_to_particle_df(hashed_particle_df: pd.DataFrame, particleHashClassDict: dict, verbose: bool = True) -> pd.DataFrame\n",
    "    - add_classID_to_particle_df(hashed_particle_df: pd.DataFrame, classIDdict: dict, verbose: bool = True) -> pd.DataFrame\n",
    "    - add_filamentID_to_particle_df(hashed_particle_df: pd.DataFrame, filamentIDdict: dict, verbose: bool = True) -> pd.DataFrame\n",
    "    - add_particleID_to_particle_df(hashed_particle_df: pd.DataFrame, particleIDdict: dict, verbose: bool = True) -> pd.DataFrame\n",
    "- Manipulating and calculating counts_df s\n",
    "    - compute_particle_counts_df(hashed_particle_df: pd.DataFrame, classIDdict: dict = None, filamentIDdict: dict = None, verbose: bool = True) -> (pd.DataFrame, pd.DataFrame, Counter, dict, Counter, dict)\n",
    "- Plotting\n",
    "    - clusterplot_counts_df(counts_df: pd.DataFrame, filepath: str = None, savefig: bool = True, dpi: int = 300, metric: str = 'cosine', vmax: int = None, cmap: str = 'inferno', standardize: int = None, row_colors: list = None, col_colors: list = None, row_linkage: np.ndarray = None, col_linkage: np.ndarray = None, figsize_x: int = 25, figsize_y: int = 25, label_fontsize: int = 15, dendrogram_ratio: float = 0.2, colors_ratio: float = 0.03, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15) -> sns.matrix.ClusterGrid\n",
    "    - dimensionality_reduction_plot(counts_df, mode: str = \"UMAP\", savefig: bool = True, filepath: str = None, figsize_x: float = 25, figsize_y: float = 25, subfigure_label_fontsize = 25, title_fontsize = 30, axis_label_fontsize = 20, alpha = 0.1, row_colors: list = None, col_colors: list = None, random_seed: int = 365)\n",
    "    - plot_class_averages(class_mrcs_file: str, frame_order: list = None, classes_df: pd.DataFrame = None, savefig: bool = True, filepath: str = None, dpi: int = 300, number_of_images: int = None, images_per_row: int = 10, label_x: int = 15, label_y: int = 225, label_fontsize: int = 10, figsize_x: float = 25, color_list: list = None, panel_label: bool = False, panel_label_letter: str = \"c\",  panel_label_fontsize: int = 15, panel_x = 0.013333, panel_y = 0.98)\n",
    "- Computing and using clusterings\n",
    "    - typecluster_initial_clustering(particles_star_file: str, output_path: str = None, savefig: bool = True, verbose: bool = True, metric: str = \"cosine\",  vmax: int = None, cmap: str = \"inferno\", standardize: int = None, figsize_x: int = 25, figsize_y: int =25, label_fontsize: int = 15, dpi: int = 300, dendrogram_ratio: float = 0.2, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15) -> (sns.matrix.ClusterGrid, pd.DataFrame, pd.DataFrame, collections.OrderedDict, Counter, dict, Counter, dict)\n",
    "    - reorder_class_average_star_by_clustered(model_star_file: str, clustered: sns.matrix.ClusterGrid, classIDdict: dict, output_path: str = None, savestar: bool = True, overwrite: bool = True) -> pd.DataFrame\n",
    "    - reorder_counts_df_by_clustered(counts_df: pd.DataFrame, clustered: sns.matrix.ClusterGrid) -> pd.DataFrame\n",
    "- Interactive Selection\n",
    "    - typecluster_interactive_select(counts_df: pd.DataFrame, clustered: sns.matrix.ClusterGrid, vmax: int = 30, cmap: list = px.colors.sequential.Inferno, figsize_x: int = 1000, figsize_y: int =1000) -> list\n",
    "    - typecluster_interactive_output(sliders: list, mode: str, output_path: str, clustered: sns.matrix.ClusterGrid, counts_df: pd.DataFrame, IDadded_particle_df: pd.DataFrame = None, all_data: collections.OrderedDict = None, classIDdict: dict = None, model_star_file: str = None, overwrite: bool = True, drophashes: bool = True) -> pd.DataFrame\n",
    "- (Semi-)Automated Selection\n",
    "    - typecluster_compute_kmeans_labels(counts_df, col_cluster_number: int = 10, row_cluster_number: int = 10, random_seed: int = 365) -> (list, list)\n",
    "    - typecluster_dendrogram_threshold_select(clustered: sns.matrix.ClusterGrid, threshold: float)\n",
    "    - typecluster_interactive_dendrogram_thresholding(clustered_df_name: str = \"clustered\") -> (widgets.FloatSlider, widgets.widget_box.HBox)\n",
    "    - typecluster_compute_dendrogram_threshold_labels(clustered: sns.matrix.ClusterGrid, threshold: float, output_path: str, counts_df: pd.DataFrame, savefig: bool = True, vmax: int = None, cmap: str = \"inferno\", standardize: int = None, figsize_x: int = 25, figsize_y: int = 25, label_fontsize: int = 15, dpi: int = 300, dendrogram_ratio: float = 0.2, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15) -> (list, Counter, sns.matrix.ClusterGrid)\n",
    "    - convert_labels_to_colors(labels, color_dict: dict = None, plot_legend: bool = True, verbose: bool = True) -> list\n",
    "    - typecluster_output_filaments_from_labels(col_labels: list, output_path: str, IDadded_particle_df: pd.DataFrame, all_data: collections.OrderedDict, particle_threshold: int = 1000, overwrite: bool = True, drophashes: bool = True, verbose: bool = True) -> pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50613c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for file input/output\n",
    "def read_particle_star(filename: str) -> (pd.DataFrame, collections.OrderedDict):\n",
    "    '''\n",
    "    Reads in particle star files, writes out a DataFrames with particles only and and an OrderedDict with all data\n",
    "    '''\n",
    "    if filename[-5:] != \".star\":\n",
    "        raise ValueError(f\"{filename} is not in .star format\")\n",
    "    else:\n",
    "        try:\n",
    "            all_data = starfile.read(filename).copy()\n",
    "            particle_df = all_data[\"particles\"].copy()\n",
    "        except:\n",
    "            raise ValueError(f\"Particle/data .star file {filename} does not contain particles or does not exist\")\n",
    "        return particle_df, all_data\n",
    "\n",
    "def read_models_star(filename: str) -> (pd.DataFrame, collections.OrderedDict):\n",
    "    '''\n",
    "    Reads in models star files, writes out a DataFrames with model classes only and and an OrderedDict with all data\n",
    "    '''\n",
    "    if filename[-5:] != \".star\":\n",
    "        raise ValueError(\"File is not in .star format\")\n",
    "    else:\n",
    "        try:\n",
    "            all_data = starfile.read(filename).copy()\n",
    "            models_df = all_data[\"model_classes\"].copy()\n",
    "        except:\n",
    "            raise ValueError(f\"Models .star file {filename} does not contain 2D classes or does not exist\")\n",
    "        return models_df, all_data\n",
    "    \n",
    "def read_mrcs_file(filename: str) -> (list, np.recarray):\n",
    "    '''\n",
    "    Reads in .mrcs files, writes out a list containing the frames and a Numpy recarray containing voxel sizes \n",
    "    '''\n",
    "    if filename[-5:] != \".mrcs\":\n",
    "        raise ValueError(f\"{filename} is not in .mrcs format\")\n",
    "    else:\n",
    "        data = []\n",
    "        try:\n",
    "            with mrcfile.open(filename,permissive=True) as mrcs:\n",
    "                for i, frame in enumerate(mrcs.data):\n",
    "                    data.append(frame)\n",
    "                voxel_size = mrcs.voxel_size\n",
    "        except:\n",
    "            raise ValueError(f\"Unable to parse .mrcs file {filename}\")\n",
    "        return data, voxel_size\n",
    "    \n",
    "def read_mrc_file(filename: str) -> (np.ndarray, np.recarray):\n",
    "    '''\n",
    "    Reads in .mrc files, writes out a Numpy ndarray containing the data and a Numpy recarray containing voxel sizes \n",
    "    '''\n",
    "    if filename[-4:] != \".mrc\":\n",
    "        raise ValueError(f\"{filename} is not in .mrc format\")\n",
    "    else:\n",
    "        try:\n",
    "            with mrcfile.open(filename,permissive=True) as mrc:\n",
    "                data = mrc.data\n",
    "                voxel_size = mrc.voxel_size\n",
    "        except:\n",
    "            raise ValueError(f\"Unable to parse .mrc file {filename}\")\n",
    "        return data, voxel_size\n",
    "    \n",
    "def write_particle_star(all_data: collections.OrderedDict, particle_df: pd.DataFrame, filename: str, overwrite: bool = True):\n",
    "    '''\n",
    "    Takes in the data_df as an OrderedDict and a DataFrame of particles and writes out a particle.star combining the two\n",
    "    '''\n",
    "    data_output = all_data.copy()\n",
    "    data_output[\"particles\"] = particle_df\n",
    "    starfile.write(data_output, f\"{filename}_particles.star\", overwrite = overwrite)\n",
    "    print(\"Particles saved as \" + f\"{filename}_particles.star\")\n",
    "\n",
    "def write_models_star(classes_df: pd.DataFrame, filename: str, overwrite: bool = True):\n",
    "    '''\n",
    "    Takes in classes as a DataFrame and writes out a classes.star\n",
    "    '''\n",
    "    starfile.write(classes_df, f\"{filename}_classes.star\", overwrite = overwrite)\n",
    "    print(\"Class averages saved as \" + f\"{filename}_classes.star\")\n",
    "    \n",
    "# Functions for adding columns to particle_df\n",
    "def hash_particle_df(particle_df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Adds a particle and filament hash to a copy of the input dataframe, replacing old hashes\n",
    "    Optionally prints out total number of filaments and total particles\n",
    "    '''\n",
    "    hashed_particle_df = particle_df.copy()\n",
    "    \n",
    "    # Remove old hashes\n",
    "    if \"Hash ID\" in hashed_particle_df.columns: # From old versions of FiTSuite, so keep for back compatibility\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'Hash ID' column in hashed_particle_df\")\n",
    "        hashed_particle_df = hashed_particle_df.drop(columns = [\"Hash ID\"])\n",
    "    if \"Hash\" in hashed_particle_df.columns: # From old versions of FiTSuite, so keep for back compatibility\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'Hash' column in hashed_particle_df\")\n",
    "        hashed_particle_df = hashed_particle_df.drop(columns = [\"Hash\"])\n",
    "    if \"filamentHash\" in hashed_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'filamentHash' column in hashed_particle_df\")\n",
    "        hashed_particle_df = hashed_particle_df.drop(columns = [\"filamentHash\"])\n",
    "    if \"particleHash\" in hashed_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'particleHash' column in hashed_particle_df\")\n",
    "        hashed_particle_df = hashed_particle_df.drop(columns = [\"particleHash\"])\n",
    "        \n",
    "    # Add in hashes\n",
    "    try:\n",
    "        hashed_particle_df[\"filamentHash\"] = hashed_particle_df[\"rlnHelicalTubeID\"].astype(str) + hashed_particle_df[\"rlnMicrographName\"]\n",
    "        hashed_particle_df[\"particleHash\"] = hashed_particle_df[\"rlnHelicalTubeID\"].astype(str) + hashed_particle_df[\"rlnMicrographName\"] + hashed_particle_df[\"rlnHelicalTrackLengthAngst\"].astype(str)\n",
    "    except:\n",
    "        raise KeyError(\"Unable to hash particle_df as not all required fields exist\")\n",
    "        \n",
    "    total_filaments = hashed_particle_df[\"filamentHash\"].nunique()\n",
    "    total_particles = hashed_particle_df[\"particleHash\"].nunique()\n",
    "    assert total_particles == len(hashed_particle_df.index), f\"particle_df contains duplicate particles\"\n",
    "    if verbose:\n",
    "        print(f\"Total filaments: {total_filaments} and total particles: {total_particles}\")\n",
    "        \n",
    "    return hashed_particle_df\n",
    "\n",
    "def add_classnumber_to_particle_df(hashed_particle_df: pd.DataFrame, particleHashClassDict: dict, verbose: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes in a hashed_particle_df and adds the rlnClassNumber of each particle from a dict to a copy of the DataFrame\n",
    "    Replaces old rlnClassNumber if it exists\n",
    "    '''\n",
    "    classadded_particle_df = hashed_particle_df.copy()\n",
    "    \n",
    "    # Remove old labels\n",
    "    if  'rlnClassNumber' in classadded_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'rlnClassNumber' column in classadded_particle_df\")\n",
    "        classadded_particle_df = classadded_particle_df.drop(columns = [\"rlnClassNumber\"])\n",
    "    \n",
    "    # Add in classes\n",
    "    try:\n",
    "        classadded_particle_df['rlnClassNumber']=classadded_particle_df['particleHash'].map(particleHashClassDict)\n",
    "    except:\n",
    "        raise KeyError(\"Unable to assign classes as not all particle assignments exist or particle_df not hashed\")\n",
    "     \n",
    "    total_classes = classadded_particle_df['rlnClassNumber'].nunique()\n",
    "    if verbose:\n",
    "        print(\"Total classes:\", total_classes)\n",
    "        \n",
    "    return classadded_particle_df\n",
    "\n",
    "def add_classID_to_particle_df(hashed_particle_df: pd.DataFrame, classIDdict: dict, verbose: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes in a hashed_particle_df and adds the classID of each particle from a dict to a copy of the DataFrame\n",
    "    Replaces old classID if it exists\n",
    "    '''\n",
    "    classIDadded_particle_df = hashed_particle_df.copy()\n",
    "    \n",
    "    # Remove old labels\n",
    "    if 'classID' in classIDadded_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'classID' column in classIDadded_particle_df\")\n",
    "        classIDadded_particle_df = classIDadded_particle_df.drop(columns = [\"classID\"])\n",
    "    \n",
    "    # Add in classes\n",
    "    try:\n",
    "        classIDadded_particle_df['classID'] = classIDadded_particle_df['rlnClassNumber'].map(classIDdict)\n",
    "    except:\n",
    "        raise KeyError(\"Unable to assign class IDs as not all class assignments exist or particle_df does not class numbers\")\n",
    "     \n",
    "    total_classIDs = classIDadded_particle_df['classID'].nunique()\n",
    "    if verbose:\n",
    "        print(\"Total class IDs:\", total_classIDs)\n",
    "        \n",
    "    return classIDadded_particle_df\n",
    "\n",
    "def add_filamentID_to_particle_df(hashed_particle_df: pd.DataFrame, filamentIDdict: dict, verbose: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes in a hashed_particle_df and adds the filamentID of each particle from a dict to a copy of the DataFrame\n",
    "    Replaces old filamentID if it exists\n",
    "    '''\n",
    "    filamentIDadded_particle_df = hashed_particle_df.copy()\n",
    "    \n",
    "    # Remove old labels\n",
    "    if  'filamentID' in filamentIDadded_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'filamentID' column in filamentIDadded_particle_df\")\n",
    "        filamentIDadded_particle_df = filamentIDadded_particle_df.drop(columns = [\"filamentID\"])\n",
    "    \n",
    "    # Add in classes\n",
    "    try:\n",
    "        filamentIDadded_particle_df['filamentID']=filamentIDadded_particle_df['filamentHash'].map(filamentIDdict)\n",
    "    except:\n",
    "        raise KeyError(\"Unable to assign filament IDs as not all particle assignments exist or particle_df not hashed\")\n",
    "     \n",
    "    total_filamentIDs = filamentIDadded_particle_df['filamentID'].nunique()\n",
    "    if verbose:\n",
    "        print(\"Total filament IDs:\", total_filamentIDs)\n",
    "        \n",
    "    return filamentIDadded_particle_df\n",
    "\n",
    "def add_particleID_to_particle_df(hashed_particle_df: pd.DataFrame, particleIDdict: dict, verbose: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes in a hashed_particle_df and adds the particleID of each particle from a dict to a copy of the DataFrame\n",
    "    Replaces old particelID if it exists\n",
    "    '''\n",
    "    particleIDadded_particle_df = hashed_particle_df.copy()\n",
    "    \n",
    "    # Remove old labels\n",
    "    if  'particleID' in particleIDadded_particle_df.columns:\n",
    "        if verbose:\n",
    "            print(\"Dropping old 'particleID' column in particleIDadded_particle_df\")\n",
    "        particleIDadded_particle_df = particleIDadded_particle_df.drop(columns = [\"particleID\"])\n",
    "    \n",
    "    # Add in classes\n",
    "    try:\n",
    "        particleIDadded_particle_df['particleID']=particleIDadded_particle_df['particleHash'].map(filamentIDdict)\n",
    "    except:\n",
    "        raise KeyError(\"Unable to assign particle IDs as not all particle assignments exist or particle_df not hashed\")\n",
    "     \n",
    "    total_particleIDs = particleIDadded_particle_df['particleID'].nunique()\n",
    "    if verbose:\n",
    "        print(\"Total particle IDs:\", total_particleIDs)\n",
    "        \n",
    "    return particleIDadded_particle_df\n",
    "\n",
    "# Functions for computing or manipulating counts_df s\n",
    "def compute_particle_counts_df(hashed_particle_df: pd.DataFrame, classIDdict: dict = None, filamentIDdict: dict = None, \n",
    "                               verbose: bool = True) -> (pd.DataFrame, pd.DataFrame, Counter, dict, Counter, dict):\n",
    "    '''\n",
    "    Computes (class x filament) dataframe of counts of particles per class per filament given a hashed_particle_df\n",
    "    If provided with a previously computed classIDdict or filamentIDdict can use those too\n",
    "    Also outputs other useful info - counts of particles per class/filament and dicts for filament/class hash to ID lookup\n",
    "    '''\n",
    "    # Check that input is valid\n",
    "    assert \"rlnClassNumber\" in hashed_particle_df.columns, \"hashed_particle_df does not contain class numbers\"\n",
    "    assert \"filamentHash\" in hashed_particle_df.columns, \"hashed_particle_df does not contain filament hashes\"\n",
    "    \n",
    "    # Compute dimensions of matrix and make up dicts to calculate IDs from\n",
    "    total_classes = hashed_particle_df[\"rlnClassNumber\"].nunique()\n",
    "    total_filaments = hashed_particle_df[\"filamentHash\"].nunique()\n",
    "    total_particles = len(hashed_particle_df.index)\n",
    "    classcount = Counter(hashed_particle_df[\"rlnClassNumber\"].to_list())\n",
    "    if classIDdict == None:\n",
    "        classIDdict = {val:idx for idx, val in enumerate(sorted(classcount.keys()))}\n",
    "    filamentcount = Counter(hashed_particle_df[\"filamentHash\"].to_list())\n",
    "    if filamentIDdict == None:\n",
    "        filamentIDdict = {val:idx for idx, val in enumerate(filamentcount.keys())}\n",
    "    if verbose:\n",
    "        print(f\"Total classes = {total_classes}, total particles = {total_particles} and total filaments = {total_filaments}\")\n",
    "        print(\"Particles per class: \", classcount)\n",
    "    \n",
    "    # Add filament and class IDs to hashed particles\n",
    "    IDadded_particle_df = add_classID_to_particle_df(hashed_particle_df, classIDdict, verbose = False)\n",
    "    IDadded_particle_df = add_filamentID_to_particle_df(IDadded_particle_df, filamentIDdict, verbose = False)\n",
    "\n",
    "    #Compute counts_df (note that pivot table approach runs much slower)\n",
    "    # counts_df = pd.pivot_table(IDadded_particle_df, values='particleHash', index='classID', columns='filamentID', aggfunc=pd.Series.nunique, fill_value = 0)\n",
    "    countsmatrix = np.zeros((len(classIDdict),len(filamentIDdict)))\n",
    "    for x, y in zip(IDadded_particle_df[\"classID\"],IDadded_particle_df[\"filamentID\"]):\n",
    "        countsmatrix[x, y] += 1\n",
    "    \n",
    "    # Making row and col labels strings facilitates plotting, row labels are original class numbers\n",
    "    counts_df = pd.DataFrame(countsmatrix, index = [str(x) for x in classIDdict.keys()], columns = [str(x) for x in range(len(filamentIDdict))])\n",
    "    counts_df.index.name = \"rlnClassNumber\"\n",
    "    counts_df.columns.name = \"filamentID\"\n",
    "    \n",
    "    return counts_df, IDadded_particle_df, classcount, classIDdict, filamentcount, filamentIDdict\n",
    "\n",
    "# Functions for plotting figures\n",
    "def clusterplot_counts_df(counts_df: pd.DataFrame, filepath: str = None, savefig: bool = True, dpi: int = 300,\n",
    "                          metric: str = 'cosine', vmax: int = None, cmap: str = 'inferno', standardize: int = None,\n",
    "                          row_colors: list = None, col_colors: list = None, \n",
    "                          row_linkage: np.ndarray = None, col_linkage: np.ndarray = None, \n",
    "                          figsize_x: int = 25, figsize_y: int = 25, label_fontsize: int = 15, \n",
    "                          dendrogram_ratio: float = 0.2, colors_ratio: float = 0.03, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), \n",
    "                          panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15) -> sns.matrix.ClusterGrid:\n",
    "    '''\n",
    "    Takes in counts_df and plots clustermap using a UPGMA/average hierarchical clustering\n",
    "    Can modify various parameters and optionally save figure\n",
    "    Refer to https://seaborn.pydata.org/generated/seaborn.clustermap.html for most params\n",
    "    cmaps that are perceptually uniform sequential are viridis, plasma, inferno, magma, cividis\n",
    "    '''\n",
    "    clustered = sns.clustermap(counts_df, metric = metric, vmax = vmax, cmap = cmap, standard_scale = standardize,\n",
    "                               figsize =(figsize_x, figsize_y), row_linkage = row_linkage, col_linkage = col_linkage,\n",
    "                               row_colors = row_colors, col_colors = col_colors, cbar_kws={'label': 'Particle Counts'},\n",
    "                               dendrogram_ratio = dendrogram_ratio, colors_ratio = colors_ratio, cbar_pos=cbar_pos)\n",
    "    \n",
    "    # Add labels\n",
    "    clustered.ax_heatmap.set_ylabel(\"2D Class Average Numbers\", fontsize = label_fontsize)\n",
    "    clustered.ax_heatmap.set_xlabel(\"Filament IDs\", fontsize = label_fontsize)\n",
    "    cbar = clustered.ax_heatmap.collections[0].colorbar\n",
    "    cbar.ax.yaxis.label.set_fontsize(label_fontsize)\n",
    "    \n",
    "    # For figure plotting can add a panel label, if you do this almost certainly want to move the cbar_pos \n",
    "    # With a panel_label_fontsize of 15, cbar_pos = (0.1, 0.82, 0.05, 0.16) seems to work\n",
    "    if panel_label:\n",
    "        font = {'fontname': 'Arial', 'fontsize': panel_label_fontsize, 'fontweight': 'bold'}\n",
    "        plt.text(0.02, 0.97, panel_label_letter, fontdict = font, transform=plt.gcf().transFigure)\n",
    "    plt.show()\n",
    "    \n",
    "    if savefig:\n",
    "        if filepath != None:\n",
    "            clustered.savefig(f\"{filepath}_clustered.png\", dpi=dpi)\n",
    "            print(\"Figure saved as \"+f\"{filepath}_clustered.png\")\n",
    "        else:\n",
    "            clustered.savefig(\"counts_df_clustered.png\", dpi=dpi)\n",
    "            print(\"Figure saved as counts_df_clustered.png\")\n",
    "    \n",
    "    return clustered\n",
    "\n",
    "def dimensionality_reduction_plot(counts_df, mode: str = \"UMAP\", savefig: bool = True, filepath: str = None, \n",
    "                                figsize_x: float = 25, figsize_y: float = 25, subfigure_label_fontsize = 25, \n",
    "                                title_fontsize = 30, axis_label_fontsize = 20, alpha = 0.1,\n",
    "                                row_colors: list = None, col_colors: list = None, random_seed: int = 365):\n",
    "    '''\n",
    "    Performs dimensionality reduction on filaments or classes and plots/optionally saves figure\n",
    "    Can choose from PCA/t-SNE/UMAP, and can also color by type by providing color labels\n",
    "    Generally, no real reason to use this over the hierarchical clustering plot but it is available.\n",
    "    '''\n",
    "    # Choose right mode\n",
    "    if mode == \"PCA\":\n",
    "        optimiser = PCA(n_components=2, random_state = random_seed)\n",
    "        # Seems weird to have a seed for PCA, but it just depends on the solver used\n",
    "    elif mode == \"t-SNE\":\n",
    "        optimiser = TSNE(n_components=2, random_state = random_seed)\n",
    "    elif mode == \"UMAP\":\n",
    "        optimiser = umap.UMAP(random_state = random_seed)\n",
    "    else:\n",
    "        raise ValueError(\"Mode not defined\")\n",
    "        \n",
    "    # Compute dimensionality reduction\n",
    "    countsmatrix = np.array(counts_df)\n",
    "    ClassReduced = optimiser.fit_transform(countsmatrix)\n",
    "    ClassReducedrownormed = optimiser.fit_transform(countsmatrix/np.sum(countsmatrix, axis=1, keepdims=True))\n",
    "    FilsReduced = optimiser.fit_transform(np.transpose(countsmatrix))\n",
    "    FilsReducedcolnormed = optimiser.fit_transform(np.transpose(countsmatrix/np.sum(countsmatrix, axis=0, keepdims=True)))\n",
    "\n",
    "    # Plot figure\n",
    "    fig, axs = plt.subplots(2, 2, figsize = (figsize_x, figsize_y))\n",
    "    if row_colors == None:\n",
    "        axs[0,0].scatter(ClassReduced[:, 0], ClassReduced[:, 1], alpha = alpha, s = 100)\n",
    "        axs[0,1].scatter(ClassReducedrownormed[:, 0], ClassReducedrownormed[:, 1], alpha = alpha, s = 100)\n",
    "    else:\n",
    "        axs[0,0].scatter(ClassReduced[:, 0], ClassReduced[:, 1], alpha = alpha, s = 100, c = row_colors)\n",
    "        axs[0,1].scatter(ClassReducedrownormed[:, 0], ClassReducedrownormed[:, 1], alpha = alpha, s = 100, c = row_colors)\n",
    "    if col_colors == None:\n",
    "        axs[1,0].scatter(FilsReduced[:, 0], FilsReduced[:, 1], alpha = alpha)\n",
    "        axs[1,1].scatter(FilsReducedcolnormed[:, 0], FilsReducedcolnormed[:, 1], alpha = alpha)\n",
    "    else:\n",
    "        axs[1,0].scatter(FilsReduced[:, 0], FilsReduced[:, 1], alpha = alpha, c = col_colors)\n",
    "        axs[1,1].scatter(FilsReducedcolnormed[:, 0], FilsReducedcolnormed[:, 1], alpha = alpha, c = col_colors)\n",
    "    \n",
    "    # Add labels, adjust layout\n",
    "    axs[0,0].set_title(\"Class Averages\", fontname = 'Arial', fontsize = subfigure_label_fontsize)\n",
    "    axs[0,1].set_title(\"Normalized Class Averages\", fontname = 'Arial', fontsize = subfigure_label_fontsize)\n",
    "    axs[1,0].set_title(\"Unique Filaments\", fontname = 'Arial', fontsize = subfigure_label_fontsize)\n",
    "    axs[1,1].set_title(\"Normalized Unique Filaments\", fontname = 'Arial', fontsize = subfigure_label_fontsize)\n",
    "    for axis in [axs[0,0], axs[0,1], axs[1,0], axs[1,1]]:\n",
    "        axis.set_xlabel(f\"{mode} dimension 1\", fontname = 'Arial', fontsize = axis_label_fontsize)\n",
    "        axis.set_ylabel(f\"{mode} dimension 2\", fontname = 'Arial', fontsize = axis_label_fontsize)\n",
    "    fig.suptitle(f\"Dimensionality Reduction by {mode}\", fontname = 'Arial', fontsize=title_fontsize, y = 0.99)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1, top=0.95)\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if savefig:\n",
    "        if filepath != None:\n",
    "            fig.savefig(f\"{filepath}_dimensionalityreduction_{mode}.png\")\n",
    "            print(f\"Figure saved as {filepath}_dimensionalityreduction_{mode}.png\")\n",
    "        else:\n",
    "            fig.savefig(f\"counts_df_dimensionalityreduction_{mode}.png\")\n",
    "            print(f\"Figure saved as counts_df_dimensionalityreduction_{mode}.png\")\n",
    "            \n",
    "def plot_class_averages(class_mrcs_file: str, frame_order: list = None, classes_df: pd.DataFrame = None,\n",
    "                        savefig: bool = True, filepath: str = None, dpi: int = 300,\n",
    "                        number_of_images: int = None, images_per_row: int = 10, \n",
    "                        label_x: int = 15, label_y: int = 225,\n",
    "                        label_fontsize: int = 10, figsize_x: float = 25, color_list: list = None,\n",
    "                        panel_label: bool = False, panel_label_letter: str = \"c\",  panel_label_fontsize: int = 15,\n",
    "                        panel_x = 0.013333, panel_y = 0.98):\n",
    "    '''\n",
    "    Plots specified number of class averages from a mrcs file or the maximum possible\n",
    "    Can plot them in a specified order from a list/classes_df (frame_order prioritized over classes_df) with colors\n",
    "    \n",
    "    Params:\n",
    "        class_mrcs_file = path to mrcs.star file as string (required)\n",
    "        frame_order = list containing frames (indexed from 0) desired\n",
    "        classes_df = DataFrame, containing the frames in their desired order in the 'rlnReferenceImage' column\n",
    "        savefig = True or False, boolean flag\n",
    "        filepath = path name that will be appended to the front of anything written out\n",
    "        dpi = int, default 300\n",
    "        number_of_images = int, number of images desired, will plot less if there aren't enough\n",
    "        images_per_row = int, default 10\n",
    "        label_fontsize = int, default 10\n",
    "        figsize_x = float, default 25, width of figure, will be autoscaled\n",
    "        color_list = list of colors already in the desired frame order\n",
    "        label_x = 15\n",
    "        label_y = 225\n",
    "        panel_label: Controls whether to add panel label for figure making\n",
    "        panel_label_letter: str, default \"b\",  \n",
    "        panel_label_fontsize: int, default 15\n",
    "        panel_x = 0.013333\n",
    "        panel_y = 0.98\n",
    "    '''\n",
    "    frame_data, voxel_size = read_mrcs_file(class_mrcs_file)\n",
    "    \n",
    "    # Set up params\n",
    "    if frame_order != None:\n",
    "        frame_order_list = frame_order\n",
    "    elif classes_df is not None:\n",
    "        frame_order_list = [int(framename.split(\"@\")[0])-1 for framename in classes_df['rlnReferenceImage']]\n",
    "    else:\n",
    "        frame_order_list = list(range(0, len(frame_data)))\n",
    "        \n",
    "    if number_of_images == None:\n",
    "        n = len(frame_order_list)\n",
    "    else:\n",
    "        n = min(number_of_images, len(frame_order_list)) \n",
    "        \n",
    "    # Plot figure\n",
    "    if panel_label:\n",
    "        fig, axes = plt.subplots(int(np.ceil(n/images_per_row)), images_per_row, figsize = (figsize_x, (int(np.ceil(n/images_per_row))+0.1)*figsize_x/images_per_row))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(int(np.ceil(n/images_per_row)), images_per_row, figsize = (figsize_x, int(np.ceil(n/images_per_row))*figsize_x/images_per_row))\n",
    "    axes = axes.flatten()\n",
    "    width, height = frame_data[0].shape\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n:\n",
    "            try:\n",
    "                ax.imshow(frame_data[frame_order_list[i]], cmap=\"gray\", origin=\"lower\")\n",
    "                # sns.heatmap(frame_data[frame_order_list[i]], ax = ax, cmap = 'gray', cbar = False, square = True)\n",
    "                ax.text(label_x, label_y, str(frame_order_list[i]+1), fontname = 'Arial', fontfamily = 'sans-serif', fontsize = label_fontsize, c = 'white') \n",
    "                ax.axis('off')\n",
    "                if color_list is not None:\n",
    "                    ax.add_patch(patches.Rectangle((0, 0), width, height, fill = False, lw = 4, ec = color_list[i]))\n",
    "            except:\n",
    "                ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    if panel_label:\n",
    "        font = {'fontname': 'Arial', 'fontsize': panel_label_fontsize, 'fontweight': 'bold'}\n",
    "        fig.suptitle(panel_label_letter, x = panel_x, y = panel_y, fontname = 'Arial', fontsize = panel_label_fontsize, fontweight = 'bold') #fontdict = font)\n",
    "        plt.tight_layout(w_pad = 0.1, h_pad = 0.1, pad = 0.4)\n",
    "    else:\n",
    "        plt.tight_layout(w_pad = 0.5) #, h_pad=0.5)  \n",
    "    plt.show()\n",
    "    \n",
    "    if savefig:\n",
    "        if filepath != None:\n",
    "            fig.savefig(f\"{filepath}_classaverages.png\", dpi=dpi)\n",
    "            print(\"Figure saved as \"+f\"{filepath}_classaverages.png\")\n",
    "        else:\n",
    "            fig.savefig(\"classaverages.png\", dpi=dpi)\n",
    "            print(\"Figure saved as classaverages.png\")\n",
    "            \n",
    "# Functions for generating clustering and using clustering\n",
    "def typecluster_initial_clustering(particles_star_file: str, output_path: str = None, savefig: bool = True, verbose: bool = True,\n",
    "                                   metric: str = \"cosine\",  vmax: int = None, cmap: str = \"inferno\", standardize: int = None, \n",
    "                                   figsize_x: int = 25, figsize_y: int =25, label_fontsize: int = 15, dpi: int = 300,\n",
    "                                   dendrogram_ratio: float = 0.2, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), \n",
    "                                   panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15\n",
    "                                   ) -> (sns.matrix.ClusterGrid, pd.DataFrame, pd.DataFrame, collections.OrderedDict, Counter, dict, Counter, dict):\n",
    "    '''\n",
    "    Computes 2D class x unique filament particle count matrix\n",
    "    Plots and saves initial hierarchical clustering figure with distograms and performs other useful calculations\n",
    "    If saved, hierachical clustering will be saved as {output_path}_clustered.png or \"counts_df_clustered.png\"\n",
    "    \n",
    "    General params:\n",
    "        particles_star_file = path to particle/data.star file as string, needs to have rlnClassNumbers assigned\n",
    "                              e.g. from Select or Class2D jobs (required!)\n",
    "        savefig = True or False, boolean flag\n",
    "        output_path = path name that will be appended to the front of anything written out\n",
    "        verbose = True or False, boolean flag for extra text output\n",
    "    \n",
    "    Clustering Figure params (all optional):\n",
    "        metric = distance metric as string, default is \"cosine\", \"jaccard\" and \"euclidean\" also can work well\n",
    "        savefig = bool, default True, controls whether figures are saved\n",
    "        vmax = int, maximum value for colormap in hierarchical clustering e.g. 15\n",
    "        cmap = color map as string, default \"inferno\", ideally perceptually uniform sequential cmaps\n",
    "        standardize = int, normalize counts matrix by row (0) or column (1)\n",
    "        figsize_x, figsize_y = ints, default 25, figure dimensions to be saved\n",
    "        label_fontsize: int, default 15, \n",
    "        dpi: int, default 300,\n",
    "        dendrogram_ratio: float controlling ratio of plot that is dendrogram \n",
    "        cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), \n",
    "        panel_label: Controls whether to add panel label for figure making\n",
    "        panel_label_letter: str, default \"a\",  \n",
    "        panel_label_fontsize: int, default 15\n",
    "    '''\n",
    "    # Read in relevant data and calculate class x filament particle count matrix\n",
    "    particle_df, all_data = read_particle_star(particles_star_file)\n",
    "    hashed_particle_df = hash_particle_df(particle_df, verbose = False)\n",
    "    counts_df, IDadded_particle_df, classcount, classIDdict, filamentcount, filamentIDdict = compute_particle_counts_df(hashed_particle_df, verbose = verbose) \n",
    "    \n",
    "    # Compute and plot hierarchical clustering\n",
    "    clustered = clusterplot_counts_df(counts_df, filepath = output_path, savefig = savefig, dpi = dpi,\n",
    "                                      metric = metric, vmax = vmax, cmap = cmap, standardize = standardize,\n",
    "                                      figsize_x = figsize_x, figsize_y = figsize_y, label_fontsize = label_fontsize, \n",
    "                                      dendrogram_ratio = dendrogram_ratio, cbar_pos = cbar_pos, \n",
    "                                      panel_label = panel_label, panel_label_letter = panel_label_letter,  panel_label_fontsize = panel_label_fontsize)\n",
    "    \n",
    "    return clustered, counts_df, IDadded_particle_df, all_data, classcount, classIDdict, filamentcount, filamentIDdict\n",
    "\n",
    "def reorder_class_average_star_by_clustered(model_star_file: str, clustered: sns.matrix.ClusterGrid, classIDdict: dict,\n",
    "                                            output_path: str = None, savestar: bool = True, overwrite: bool = True\n",
    "                                           ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Reorders class averages in a model_classes_df and appends a rlnClassNumber and clusteredOrder column\n",
    "    Writes out if desired, and can be visualized by plot_class_averages\n",
    "    \n",
    "    Params:\n",
    "        model_star_file = path to model.star file as string, required if classes are to be written out\n",
    "        clustered = sns.matrix.ClusterGrid, output e.g. from typecluster_initial_clustering\n",
    "        classIDdict = dict, output e.g. from typecluster_initial_clustering\n",
    "        output_path = path name that will be appended to the front of anything written out\n",
    "        savestar = bool, default True, controls whether reordered star file will be written out\n",
    "        overwrite = bool, default True, controls whether it is okay to overwrite existing star\n",
    "    '''\n",
    "    # Read in order\n",
    "    ordered_row_indices = clustered.dendrogram_row.reordered_ind\n",
    "    classes_to_keep_dict = {classID:ordered_row_indices.index(classIDdict[classID]) for classID in classIDdict.keys()}\n",
    "\n",
    "    # Read in and add columns to model_star\n",
    "    model_classes_df, all_model_data = read_models_star(model_star_file)\n",
    "    reordered_classes_df = model_classes_df.copy()\n",
    "    reordered_classes_df['rlnClassNumber'] = list(np.arange(1,len(reordered_classes_df.index)+1))\n",
    "    \n",
    "    # Filter 2D class files with new order and write out\n",
    "    reordered_classes_df = reordered_classes_df[reordered_classes_df['rlnClassNumber'].isin(set(classes_to_keep_dict.keys()))]\n",
    "    reordered_classes_df['clusteredOrder'] = reordered_classes_df['rlnClassNumber'].map(classes_to_keep_dict)\n",
    "    reordered_classes_df = reordered_classes_df.sort_values(by=['clusteredOrder'])\n",
    "    \n",
    "    if savestar:\n",
    "        if output_path == None:\n",
    "            output_path = \"clustered\"\n",
    "        write_models_star(reordered_classes_df, filename = f\"{output_path}_reordered\", overwrite = overwrite)\n",
    "    \n",
    "    return reordered_classes_df\n",
    "\n",
    "def reorder_counts_df_by_clustered(counts_df: pd.DataFrame, clustered: sns.matrix.ClusterGrid) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes in a counts_df and a clustermap and rearranges counts_df to match order\n",
    "    '''\n",
    "    new_cols = counts_df.columns[clustered.dendrogram_col.reordered_ind]\n",
    "    new_ind = counts_df.index[clustered.dendrogram_row.reordered_ind]\n",
    "    reordered_counts_df = counts_df.loc[new_ind, new_cols]\n",
    "    return reordered_counts_df\n",
    "\n",
    "# Functions for interactive selection, visualization and output\n",
    "def typecluster_interactive_select(counts_df: pd.DataFrame, clustered: sns.matrix.ClusterGrid, vmax: int = 30, \n",
    "                                   cmap: list = px.colors.sequential.Inferno, figsize_x: int = 1000, \n",
    "                                   figsize_y: int =1000) -> list:\n",
    "    '''\n",
    "    Replots hierarchically ordered counts_df using plotly, allowing for interactive selection of params\n",
    "    Parameters are then passed to an typecluster_interactive_output function\n",
    "    \n",
    "    Params:\n",
    "        counts_df = DataFrame containing counts, e.g. from typecluster_initial_clustering\n",
    "        clustered = Clustermap, e.g. from typecluster_initial_clustering\n",
    "        vmax = int, maximum value for colormap in hierarchical clustering e.g. 15\n",
    "        cmap = color map as list from plotly, default px.colors.sequential.Inferno\n",
    "        figsize_x, figsize_y = ints, default 1000, figure dimensions to be saved as pixels\n",
    "    '''\n",
    "    # Compute new df that is reordered by the clustering\n",
    "    clustered_df = reorder_counts_df_by_clustered(counts_df, clustered)\n",
    "    clustered_df.index = [clustered_df.index[x]+\"_\"+str(x) for x in range(len(clustered_df.index))]\n",
    "    clustered_df.columns = [clustered_df.columns[x]+\"_\"+str(x) for x in range(len(clustered_df.columns))]\n",
    "    \n",
    "    # Set up sliders\n",
    "    ynum, xnum = clustered_df.shape\n",
    "    x1 = widgets.IntSlider(value=1, min=0.0, max=xnum-1, step=1.0, description='x-start:', continuous_update=False)\n",
    "    y1 = widgets.IntSlider(value=1, min=0.0, max=ynum-1, step=1.0, description='y-start:', continuous_update=False)\n",
    "    x2 = widgets.IntSlider(value=2, min=0.0, max=xnum-1, step=1.0, description='x-end:', continuous_update=False)\n",
    "    y2 = widgets.IntSlider(value=2, min=0.0, max=ynum-1, step=1.0, description='y-end:', continuous_update=False)\n",
    "    container = widgets.HBox(children=[x1, y1, x2, y2])\n",
    "\n",
    "    # Set up figure\n",
    "    fig = px.imshow(clustered_df, aspect = 'auto', labels=dict(y=\"2D Class Number (name + coordinate #)\", x=\"Filament Hash ID (name + coordinate #)\", color=\"Counts\"),\n",
    "                   color_continuous_scale=cmap, range_color = [0,vmax])\n",
    "    fig.add_selection(x0=1, y0=1, x1=2, y1=2, line=dict(color=\"White\",width=1,dash=\"dash\"))\n",
    "    fig.update_layout(dragmode = \"zoom\")\n",
    "    fig.update_layout(height = figsize_x, width = figsize_y)\n",
    "    interactive_fig = go.FigureWidget(fig)\n",
    "\n",
    "    # Set up interaction\n",
    "    def response(change):\n",
    "        with interactive_fig.batch_update():\n",
    "            interactive_fig.layout.selections[0]['x0'] = x1.value\n",
    "            interactive_fig.layout.selections[0]['x1'] = x2.value\n",
    "            interactive_fig.layout.selections[0]['y0'] = y1.value\n",
    "            interactive_fig.layout.selections[0]['y1'] = y2.value\n",
    "\n",
    "    x1.observe(response, names=\"value\")\n",
    "    x2.observe(response, names=\"value\")\n",
    "    y1.observe(response, names=\"value\")\n",
    "    y2.observe(response, names=\"value\")\n",
    "    \n",
    "    # Show figure\n",
    "    sliders = [x1, x2, y1, y2]\n",
    "    output = widgets.VBox([container, interactive_fig])\n",
    "    display(output)\n",
    "    \n",
    "    return sliders\n",
    "    \n",
    "def typecluster_interactive_output(sliders: list, mode: str, output_path: str, clustered: sns.matrix.ClusterGrid, \n",
    "                                   counts_df: pd.DataFrame, IDadded_particle_df: pd.DataFrame = None, all_data: collections.OrderedDict = None,\n",
    "                                   classIDdict: dict = None, model_star_file: str = None, overwrite: bool = True,\n",
    "                                   drophashes: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    \n",
    "    Params:\n",
    "        sliders: list of 4 sliders from typecluster_interactive_select\n",
    "        mode = can be \"Filaments\", \"Classes\", or \"Particles\" depending on what is to be written out\n",
    "        output_path = path name that will be appended to the front of anything written out (e.g. /TypeClassifer/group1)\n",
    "        counts_df = DataFrame containing counts, e.g. from typecluster_initial_clustering\n",
    "        clustered = Clustermap, e.g. from typecluster_initial_clustering\n",
    "        IDadded_particle_df = particle_df with filament and class IDs, e.g. from \n",
    "        all_data = original read in of star file\n",
    "        model_star_file = path for model.star file, if mode = \"Classes\"\n",
    "        overwrite = boolean that controls whether it is okay to overwrite existing star files\n",
    "    '''\n",
    "    # Extract values from sliders\n",
    "    [x1, x2, y1, y2] = sliders\n",
    "    class_start = min(y1.value, y2.value)\n",
    "    class_end = max(y1.value, y2.value)\n",
    "    hash_start = min(x1.value, x2.value)\n",
    "    hash_end = max(x1.value, x2.value)\n",
    "    ynum, xnum = counts_df.shape\n",
    "    \n",
    "    # Find indices corresponding to these regions\n",
    "    if class_start != class_end:\n",
    "        if class_end != ynum-1:\n",
    "            selected_2D_class_inds = clustered.dendrogram_row.reordered_ind[class_start:class_end+1]\n",
    "        else:\n",
    "            selected_2D_class_inds = clustered.dendrogram_row.reordered_ind[class_start:]        \n",
    "    else:\n",
    "        selected_2D_class_inds = clustered.dendrogram_row.reordered_ind[class_start]\n",
    "        \n",
    "    if hash_start != hash_end:\n",
    "        if hash_end != xnum-1:\n",
    "            selected_hash_inds = clustered.dendrogram_col.reordered_ind[hash_start:hash_end+1]\n",
    "        else:\n",
    "            selected_hash_inds = clustered.dendrogram_col.reordered_ind[hash_start:]        \n",
    "    else:\n",
    "        selected_hash_inds = clustered.dendrogram_col.reordered_ind[hash_start]\n",
    "    \n",
    "    # Write df that only contains selected particles from clustered matrix\n",
    "    all_selected_df = counts_df.loc[counts_df.index[selected_2D_class_inds], counts_df.columns[selected_hash_inds]]\n",
    "    selected_classes = set([int(x) for x in all_selected_df.index])\n",
    "    selected_filaments = set([int(x) for x in all_selected_df.columns])\n",
    "    \n",
    "    # Write out correct type of file\n",
    "    if mode == \"Classes\": # Writing out all selected 2D classes\n",
    "        assert (model_star_file is not None and classIDdict is not None), \"Not all required inputs provided\"\n",
    "        filtered_df = reorder_class_average_star_by_clustered(model_star_file, clustered, classIDdict, savestar = False)\n",
    "        filtered_df = filtered_df[filtered_df['rlnClassNumber'].isin(selected_classes)]\n",
    "        write_models_star(filtered_df, f\"{output_path}_selected\", overwrite = overwrite)\n",
    "    elif mode == \"Filaments\": # Writing out all particles from selected filaments\n",
    "        assert (IDadded_particle_df is not None and all_data is not None), \"Not all required inputs provided\"\n",
    "        filtered_df = IDadded_particle_df.copy()\n",
    "        filtered_df = filtered_df[filtered_df[\"filamentID\"].isin(selected_filaments)]\n",
    "        if drophashes:\n",
    "            try:\n",
    "                columns_to_drop = list(set([\"filamentID\", \"particleID\", \"filamentHash\", \"particleHash\", \"classID\"]).intersection(filtered_df.columns))\n",
    "                filtered_df = filtered_df.drop(columns = columns_to_drop)\n",
    "            except:\n",
    "                print(\"Unable to drop hashes\")\n",
    "        write_particle_star(all_data, filtered_df, f\"{output_path}_selected_filament\", overwrite = overwrite)\n",
    "    elif mode == \"Particles\":\n",
    "        assert (IDadded_particle_df is not None and all_data is not None), \"Not all required inputs provided\"\n",
    "        filtered_df = IDadded_particle_df.copy()\n",
    "        filtered_df = filtered_df[filtered_df[\"filamentID\"].isin(selected_filaments)&filtered_df[\"rlnClassNumber\"].isin(selected_classes)]\n",
    "        if drophashes:\n",
    "            try:\n",
    "                columns_to_drop = list(set([\"filamentID\", \"particleID\", \"filamentHash\", \"particleHash\", \"classID\"]).intersection(filtered_df.columns))\n",
    "                filtered_df = filtered_df.drop(columns = columns_to_drop)\n",
    "            except:\n",
    "                print(\"Unable to drop hashes\")\n",
    "        write_particle_star(all_data, filtered_df, f\"{output_path}_selected\", overwrite = overwrite)\n",
    "    else:\n",
    "        raise ValueError(\"Current mode is undefined. Please choose from 'Classes', 'Filaments', or 'Particles'\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Functions for automated cluster selection, visualization, and output\n",
    "def typecluster_compute_kmeans_labels(counts_df, col_cluster_number: int = 10, row_cluster_number: int = 10, random_seed: int = 365) -> (list, list): \n",
    "    '''\n",
    "    Compute k-means labels for class averages (row_clusters) and unique filaments (col_clusters)\n",
    "    Returns a list for each containing the cluster number of each\n",
    "    '''\n",
    "    countsmatrix = np.array(counts_df)\n",
    "    # Compute k-means on rows without normalization, but could in theory do /np.sum(countsmatrix, axis=0, keepdims=True))\n",
    "    row_labels = KMeans(n_clusters=row_cluster_number, random_state=random_seed, n_init = 'auto').fit_predict(countsmatrix) \n",
    "    # Column k-means done with normalization\n",
    "    countsmatrix_colnormalized = countsmatrix/np.sum(countsmatrix, axis=0, keepdims=True)\n",
    "    col_labels = KMeans(n_clusters=col_cluster_number, random_state=random_seed, n_init = 'auto').fit_predict(np.transpose(countsmatrix_colnormalized))\n",
    "    return row_labels, col_labels\n",
    "\n",
    "def typecluster_dendrogram_threshold_select(clustered: sns.matrix.ClusterGrid, threshold: float):\n",
    "    '''\n",
    "    Can try out various cosine distance thresholds here and visualize on the distogram\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize = (40, 25))\n",
    "    dendrogram(clustered.dendrogram_col.linkage, ax = ax, color_threshold = threshold, no_labels = True)\n",
    "    \n",
    "def typecluster_interactive_dendrogram_thresholding(clustered_df_name: str = \"clustered\") -> (widgets.FloatSlider, widgets.widget_box.HBox):\n",
    "    '''\n",
    "    SUPER hacky way to do interactive threshold selection on dendrograms\n",
    "    Wish it was done in a much better way but wasted too much time trying to fix a widgets implementation\n",
    "    Notably, second output needs to be hbox\n",
    "    '''\n",
    "    # Set up widgets\n",
    "    threshold_slider = widgets.FloatSlider(min=0, max=1, step=0.001, description='Threshold:', readout_format='.3f')\n",
    "    run_button = widgets.Button(description='Run')\n",
    "    \n",
    "    # Set up response\n",
    "    def run_button_clicked(button):\n",
    "        command1 = \"IPython.display.clear_output()\" # So we don't keep a bunch of plots\n",
    "        exec(command1)\n",
    "        command2 = \"display(hbox)\" # Have to regenerate sliders, and thus need to return it as well\n",
    "        exec(command2)\n",
    "        command3 = f\"typecluster_dendrogram_threshold_select({clustered_df_name}, {threshold_slider.value})\"\n",
    "        exec(command3)\n",
    "    \n",
    "    # Make widget\n",
    "    run_button.on_click(run_button_clicked)\n",
    "    hbox = widgets.HBox([threshold_slider, run_button])\n",
    "    display(hbox)\n",
    "    return threshold_slider, hbox\n",
    "\n",
    "def typecluster_compute_dendrogram_threshold_labels(clustered: sns.matrix.ClusterGrid, threshold: float, \n",
    "                                                    output_path: str, counts_df: pd.DataFrame, savefig: bool = True,\n",
    "                                                    vmax: int = None, cmap: str = \"inferno\", standardize: int = None,\n",
    "                                                    figsize_x: int = 25, figsize_y: int = 25, label_fontsize: int = 15, \n",
    "                                                    dpi: int = 300, dendrogram_ratio: float = 0.2, cbar_pos: tuple = (0.02, 0.81, 0.05, 0.17), \n",
    "                                                    panel_label: bool = False, panel_label_letter: str = \"a\",  panel_label_fontsize: int = 15\n",
    "                                                   ) -> (list, Counter, sns.matrix.ClusterGrid):\n",
    "    '''\n",
    "    Uses fcluster to find filament clusters under a minimum distance_threshold (nominally cosine distance)\n",
    "    Outputs cluster labels for filaments and a counter of filaments/cluster\n",
    "    Also plots labeled clusters on clustermap\n",
    "    \n",
    "    Params:\n",
    "        clustered = Clustermap, e.g. from typecluster_initial_clustering\n",
    "        threshold = float that is the maximum average cosine distance for a cluster\n",
    "                    probably either a float threshold or threshold_slider.value\n",
    "        counts_df = DataFrame containing counts, e.g. from typecluster_initial_clustering\n",
    "        output_path = path name that will be appended to the front of anything written out (e.g. /TypeClassifer/group1)\n",
    "        \n",
    "    Otherwise, most params the same as `typecluster_initial_clustering` for visualization purposes\n",
    "    '''\n",
    "    col_clustered_labels = fcluster(clustered.dendrogram_col.linkage, t=threshold, criterion='distance')\n",
    "    \n",
    "    # Compute figure\n",
    "    col_color_labels = convert_labels_to_colors(col_clustered_labels)\n",
    "    output_pathused = output_path+f\"_{threshold}threshold\"\n",
    "    labeled_clustered = clusterplot_counts_df(counts_df, filepath = output_pathused, savefig = savefig, dpi = dpi,\n",
    "                                              row_linkage = clustered.dendrogram_row.linkage, col_linkage = clustered.dendrogram_col.linkage,\n",
    "                                              col_colors = col_color_labels, vmax = vmax, cmap = cmap, standardize = standardize,\n",
    "                                              figsize_x = figsize_x, figsize_y = figsize_y, label_fontsize = label_fontsize, \n",
    "                                              dendrogram_ratio = dendrogram_ratio, cbar_pos = cbar_pos, \n",
    "                                              panel_label = panel_label, panel_label_letter = panel_label_letter,  panel_label_fontsize = panel_label_fontsize)\n",
    "\n",
    "    return col_clustered_labels, labeled_clustered\n",
    "\n",
    "def convert_labels_to_colors(labels, color_dict: dict = None, plot_legend: bool = True, verbose: bool = True) -> list:\n",
    "    '''\n",
    "    Converts a list of labels (ints) to a list of colors either through a specificed color_dict or by uniform sampling\n",
    "    '''\n",
    "    counted_labels = Counter(labels)\n",
    "    if verbose:\n",
    "        print(f\"Number of filaments/classes per label: {counted_labels}\")\n",
    "    unique_labels = len(counted_labels.keys())\n",
    "    \n",
    "    if color_dict != None:\n",
    "        assert len(color_dict) >= unique_labels, \"Not enough colors specified in provided color_dict\"\n",
    "        try:\n",
    "            colors = [color_dict[label] for label in labels]\n",
    "        except:\n",
    "            raise KeyError(\"Unable to convert labels to colors using provided color_dict\")\n",
    "    else:\n",
    "        colors = sns.color_palette(\"husl\", unique_labels)\n",
    "        color_dict = {label_id: colors[i] for i, label_id in enumerate(counted_labels.keys())}\n",
    "        colors = [color_dict[label] for label in labels]\n",
    "    \n",
    "    if plot_legend:\n",
    "        # Create subplots for each color\n",
    "        num_rows = int(np.ceil(unique_labels/20))\n",
    "        fig, axs = plt.subplots(num_rows, 20, figsize=(20, num_rows))\n",
    "\n",
    "        # Flatten the axs array if it has multiple dimensions\n",
    "        if isinstance(axs, np.ndarray):\n",
    "            axs = axs.flatten()\n",
    "\n",
    "        # Iterate over the color list and display each color in a subplot\n",
    "        for i, label in enumerate(sorted(counted_labels.keys())):\n",
    "            ax = axs[i]\n",
    "            ax.set_facecolor(color_dict[label])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(str(label))\n",
    "        \n",
    "        # Remove empty subplots if necessary\n",
    "        if unique_labels < len(axs):\n",
    "            for j in range(unique_labels, len(axs)):\n",
    "                axs[j].axis('off')\n",
    "        \n",
    "        # Adjust spacing between subplots\n",
    "        plt.subplots_adjust(hspace=0.5, wspace=0.1)\n",
    "        plt.show()\n",
    "    \n",
    "    return colors\n",
    "\n",
    "def typecluster_output_filaments_from_labels(col_labels: list, output_path: str, IDadded_particle_df: pd.DataFrame,\n",
    "                                             all_data: collections.OrderedDict, particle_threshold: int = 1000, \n",
    "                                             overwrite: bool = True, drophashes: bool = True, verbose: bool = True\n",
    "                                            ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Writes out .star files containing particles from clusters with particle #s exceeding a threshold\n",
    "    Clusters that fail this threshold are merged and output as one joint merged particle .star file\n",
    "    Filament clusters specified by col_labels as a list, which come from k-means or distogram thresholding\n",
    "    \n",
    "    Params:\n",
    "        col_labels = List of labels, probably from kmeans or distogram thresholding\n",
    "        output_path = path name that will be appended to the front of anything written out (e.g. /TypeClassifer/job009)\n",
    "        IDadded_particle_df = DataFrame with filamentIDs, probably from typecluster_initial_clustering\n",
    "        all_data = reading in from read_particle_star, template for output\n",
    "        particle_threshold = minimum number of particles in a filament cluster to output it standalone (float or int)\n",
    "        overwrite = boolean that controls whether it is okay to overwrite existing star files\n",
    "        drophashes = boolean that controls whether to drop hashes when writing out star files\n",
    "        verbose = boolean that controls verbosity of text output\n",
    "    '''\n",
    "    \n",
    "    # Compute some stats\n",
    "    counted_labels = Counter(col_labels)\n",
    "    num_filaments = IDadded_particle_df['filamentID'].nunique()\n",
    "    num_particles = len(IDadded_particle_df.index)\n",
    "    if verbose:\n",
    "        print(f\"Number of filaments/classes per label: {counted_labels}\")\n",
    "    \n",
    "    # Map particles to label\n",
    "    labeled_particle_df = IDadded_particle_df.copy()\n",
    "    filamentID_label_dict = {i: label for i, label in enumerate(col_clustered_labels)}\n",
    "    if 'clusterLabel' in labeled_particle_df.columns:\n",
    "        labeled_particle_df = labeled_particle_df.drop(columns = [\"clusterLabel\"])\n",
    "    labeled_particle_df['clusterLabel'] = labeled_particle_df['filamentID'].map(filamentID_label_dict)\n",
    "    \n",
    "    # Iterate through clusters and extract particles\n",
    "    failed_cluster_labels = []\n",
    "    for cluster_label in sorted(counted_labels.keys()):\n",
    "        filtered_df = labeled_particle_df.copy()\n",
    "        filtered_df = filtered_df[filtered_df['clusterLabel']==cluster_label]\n",
    "        filtered_particle_count = filtered_df.shape[0]\n",
    "        \n",
    "        # Write out if passing particle threshold, else, group with other clusters that failed\n",
    "        if filtered_particle_count >= particle_threshold:\n",
    "            if drophashes:\n",
    "                try: # We keep cluster labels but could also drop in the future, it helps keep dimensions consistent\n",
    "                    columns_to_drop = list(set([\"filamentID\", \"particleID\", \"filamentHash\", \"particleHash\", \"classID\"]).intersection(filtered_df.columns))\n",
    "                    filtered_df = filtered_df.drop(columns = columns_to_drop)\n",
    "                except:\n",
    "                    print(\"Unable to drop hashes\")\n",
    "            if verbose:\n",
    "                print(f\"{filtered_particle_count} particles from filament cluster {cluster_label}\")\n",
    "            write_particle_star(all_data, filtered_df, f\"{output_path}_cluster{cluster_label}\", overwrite = overwrite)\n",
    "        else:\n",
    "            failed_cluster_labels.append(cluster_label)\n",
    " \n",
    "    # Deal with all of the failed clusters, if any\n",
    "    num_failed_filaments = 0\n",
    "    num_failed_particles = 0\n",
    "    if failed_cluster_labels != []:\n",
    "        failed_clusters_df = labeled_particle_df.copy()\n",
    "        failed_clusters_df = failed_clusters_df[failed_clusters_df['clusterLabel'].isin(set(failed_cluster_labels))]\n",
    "        num_failed_filaments = failed_clusters_df['filamentID'].nunique()\n",
    "        num_failed_particles = len(failed_clusters_df.index)\n",
    "        if verbose:\n",
    "            print(f\"Clusters {failed_cluster_labels} did not pass particle threshold of {particle_threshold}, so {num_failed_particles} particles will be merged together.\")\n",
    "        write_particle_star(all_data, failed_clusters_df, f\"{output_path}_clustersfailed\", overwrite = overwrite)\n",
    "        \n",
    "    # Output summary statistics and output relevant df\n",
    "    if verbose:\n",
    "        print(f\"{num_failed_particles} particles out of {num_particles} total particles in merged clusters = {num_failed_particles/num_particles*100}%.\")\n",
    "        print(f\"{num_failed_filaments} filaments out of {num_filaments} total filaments in merged clusters = {num_failed_filaments/num_filaments*100}%.\")\n",
    "    return labeled_particle_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b6f1e",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "Works best with manually extracted filaments instead of auto-picked filaments as auto-picked filaments are more likely wrong/short. As such, information about filament structure is then lost. \n",
    "\n",
    "It also works better if particles are not randomly split up by a selection tasks, so all particles in any given filament are retained. Can run with a lot of particles as long as number of filaments and number of clusters are relatively low (<50,000 for fast performance on mac) as run time scales as (# of filaments choose 2). \n",
    "\n",
    "Can repeat the selection and output functions as many times as needed, just copy and paste the relevant commands.\n",
    "\n",
    "Suggest using automated selection and output on the first high-level 2D classification job, and iterating with extraction/2D classification until the data looks homogenous. Can use interactive selection/output throughout to visualize or output specific filament types/data that you care about. \n",
    "\n",
    "For LMB users: It is typically easiest to mount directly onto /cephfs or /cephfs2 and read/write into there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb683c",
   "metadata": {},
   "source": [
    "# For you to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f559281",
   "metadata": {},
   "source": [
    "Should be all set to start, can always call `help(function_of_your_choice)` on any function to have a more detailed listing of the parameter options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaadb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Key Parameters:\n",
    "particle_star = \"../ExampleData/SL251-2_job007_run_it045_data.star\" # Path to data.star file from Class2D job\n",
    "model_star = \"../ExampleData/SL251-2_job007_run_it045_model.star\" # Path to model.star file from Class2D job\n",
    "class_mrcs = \"../ExampleData/SL251-2_job007_run_it045_classes.mrcs\" # Path to classes.mrcs file from Class2D job\n",
    "output = \"../ExampleOutput/SL251-2_job007_run_it045\" # Desired output path that will be appended to all outputs\n",
    "savefig = True # Controls whether or not to save the clustered heatmap figure\n",
    "savestar = True # Controls whether or not to save the reordered class averages \n",
    "saveclassaverageplot = True # Controls whether or not to save the class average figures\n",
    "overwrite = False # Controls whether it is okay to overwrite existing .star files with the same name\n",
    "\n",
    "clustered, counts_df, IDadded_particle_df, all_data, classcount, classIDdict, filamentcount, filamentIDdict = typecluster_initial_clustering(particle_star, output_path = output, savefig = savefig, vmax = 30)\n",
    "reordered_classes_df = reorder_class_average_star_by_clustered(model_star, clustered, classIDdict, output_path = output, savestar = savestar, overwrite = overwrite)\n",
    "plot_class_averages(class_mrcs, classes_df = reordered_classes_df, savefig = saveclassaverageplot, filepath = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75572729",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 1 OF 2 FOR INTERACTIVE SELECTION/OUTPUT, COPY AND PASTE AS NEEDED\n",
    "\n",
    "Note: indexes from 0 and we don't actually need x-end > x-start or y-end > y-start\n",
    "The start and end coordinate x-y pairs are connected though\n",
    "Selections include the boxes where the dotted outlines appear\n",
    "Can save an image of your selection with the camera-looking \"Download plot as a png\" button\n",
    "'''\n",
    "\n",
    "sliders = typecluster_interactive_select(counts_df, clustered)\n",
    "# We can keep track of our selected group here: group1, x: 860-976 y: 20-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 2 OF 2 FOR INTERACTIVE SELECTION/OUTPUT, COPY AND PASTE AS NEEDED\n",
    "'''\n",
    "\n",
    "# Key Parameters:\n",
    "selection_output = output + \"_group1\" # Path to write out selection to \n",
    "saveclassaverageplot = True # Controls whether or not to save the class average figures\n",
    "overwrite = False # Controls whether it is okay to overwrite existing .star files with the same name\n",
    "# These functions also require the parameters and data structures from the initial clustering cell\n",
    "\n",
    "# Comment out any of the lines below that you do not want to output!\n",
    "print([slider.value for slider in sliders]) # Just to remember what we selected in the previous cell\n",
    "typecluster_interactive_output(sliders, \"Filaments\", selection_output, clustered, counts_df, IDadded_particle_df = IDadded_particle_df, all_data = all_data, model_star_file = model_star, classIDdict = classIDdict, overwrite = overwrite)\n",
    "typecluster_interactive_output(sliders, \"Particles\", selection_output, clustered, counts_df, IDadded_particle_df = IDadded_particle_df, all_data = all_data, model_star_file = model_star, classIDdict = classIDdict, overwrite = overwrite)\n",
    "selected_interactive_class_averages_df = typecluster_interactive_output(sliders, \"Classes\", selection_output, clustered, counts_df, model_star_file = model_star, classIDdict = classIDdict, overwrite = overwrite)\n",
    "plot_class_averages(class_mrcs, classes_df = selected_interactive_class_averages_df, savefig = saveclassaverageplot, filepath = selection_output)\n",
    "selected_interactive_class_averages_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05392c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 1 OF 3 FOR AUTOMATED SELECTION/OUTPUT - DENDROGRAM THRESHOLDING\n",
    "\n",
    "Don't mess with the naming of the variables for the output of this function!\n",
    "Choose a threshold to try, then hit run. Can repeat until it looks good\n",
    "'''\n",
    "\n",
    "threshold_slider, hbox = typecluster_interactive_dendrogram_thresholding(\"clustered\") \n",
    "\n",
    "# Looks like 0.84 is a reasonable threshold for this data for the first round of reclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ef349",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 2 OF 3 FOR AUTOMATED SELECTION/OUTPUT - DENDROGRAM THRESHOLDING\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "output = output\n",
    "savefig = True # Controls whether or not to save the colored clustered heatmap figure\n",
    "\n",
    "col_clustered_labels, labeled_clustered = typecluster_compute_dendrogram_threshold_labels(clustered, threshold_slider.value, output, counts_df, savefig = savefig, vmax = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 2.5 OF 3 FOR AUTOMATED SELECTION/OUTPUT - DENDROGRAM THRESHOLDING (OPTIONAL)\n",
    "\n",
    "Dimensionality reduction plot, run after cell 2 of 3, if ever\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "savefig = True\n",
    "output = output\n",
    "mode = \"UMAP\" # choose from \"UMAP\", \"PCA\", and \"t-SNE\"\n",
    "\n",
    "col_clustered_colors = convert_labels_to_colors(col_clustered_labels)\n",
    "dimensionality_reduction_plot(counts_df, mode = mode, savefig = savefig, filepath = output, col_colors = col_clustered_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 3 OF 3 FOR AUTOMATED SELECTION/OUTPUT - DENDROGRAM THRESHOLDING\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "output = output\n",
    "particle_threshold = 1000 # Minimum number of particles for a cluster, clusters failing threshold will be merged\n",
    "overwrite = False # Controls whether it is okay to overwrite existing .star files with the same name\n",
    "\n",
    "labeled_particle_df = typecluster_output_filaments_from_labels(col_clustered_labels, output, IDadded_particle_df, all_data, verbose = True, particle_threshold = particle_threshold, overwrite = overwrite)\n",
    "labeled_particle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eafc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 0 OF 2 FOR AUTOMATED SELECTION/OUTPUT - K-MEANS (OPTIONAL)\n",
    "\n",
    "Can use this cell if an initial clustering has not been performed\n",
    "'''\n",
    "particle_df, all_data = read_particle_star(particle_star)\n",
    "hashed_particle_df = hash_particle_df(particle_df)\n",
    "counts_df, IDadded_particle_df, classcount, classIDdict, filamentcount, filamentIDdict = compute_particle_counts_df(hashed_particle_df)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d239bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 1 OF 2 FOR AUTOMATED SELECTION/OUTPUT - K-MEANS\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "class_average_cluster_number = 5\n",
    "filament_cluster_number = 10 # Number of clusters to use for k-means on filaments\n",
    "clustering_computed = True # Set to True if clustering previously computed, else do set to false, controls plot\n",
    "kmeansoutput = output + \"kmeans\"\n",
    "savefig = True # Controls whether or not to save the colored clustered heatmap figure\n",
    "\n",
    "row_kmeans_labels, col_kmeans_labels = typecluster_compute_kmeans_labels(counts_df, col_cluster_number = filament_cluster_number, row_cluster_number = class_average_cluster_number)\n",
    "if clustering_computed: \n",
    "    row_colors = convert_labels_to_colors(row_kmeans_labels)\n",
    "    col_colors = convert_labels_to_colors(col_kmeans_labels)\n",
    "    kmeans_clusterplot = clusterplot_counts_df(counts_df, savefig = savefig, filepath = kmeansoutput, vmax = 30, figsize_x = 20, figsize_y = 20, \n",
    "                                  label_fontsize=20, cbar_pos = (0.1, 0.81, 0.05, 0.17), panel_label = True, \n",
    "                                  panel_label_fontsize = 25, row_colors = row_colors, col_colors = col_colors,\n",
    "                                  row_linkage = clustered.dendrogram_row.linkage, col_linkage = clustered.dendrogram_col.linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61757d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 1.5 OF 2 FOR AUTOMATED SELECTION/OUTPUT - K-MEANS (OPTIONAL)\n",
    "\n",
    "Optional, but makes sense to plot dimensionality reduction plot if clustering not computed\n",
    "Run after cell 1 of 2\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "savefig = True\n",
    "kmeansoutput = output + \"kmeans\"\n",
    "mode = \"UMAP\" # choose from \"UMAP\", \"PCA\", and \"t-SNE\"\n",
    "\n",
    "row_colors = convert_labels_to_colors(row_kmeans_labels)\n",
    "col_colors = convert_labels_to_colors(col_kmeans_labels)\n",
    "dimensionality_reduction_plot(counts_df, mode = mode, savefig = savefig, filepath = kmeansoutput, row_colors = row_colors, col_colors = col_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643958ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CELL 2 OF 2 FOR AUTOMATED SELECTION/OUTPUT - K-MEANS\n",
    "'''\n",
    "\n",
    "# Params:\n",
    "kmeansoutput = output + \"kmeans\" #  Path for output\n",
    "particle_threshold = 1000 # Minimum number of particles for a cluster, clusters failing threshold will be merged\n",
    "overwrite = False # Controls whether it is okay to overwrite existing .star files with the same name\n",
    "\n",
    "labeled_kmeans_particle_df = typecluster_output_filaments_from_labels(col_clustered_labels, kmeansoutput, IDadded_particle_df, all_data, verbose = True, particle_threshold = particle_threshold, overwrite = overwrite)\n",
    "labeled_kmeans_particle_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FiTSuite",
   "language": "python",
   "name": "fitsuite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
